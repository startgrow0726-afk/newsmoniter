# server.py â€” ìµœì í™”ëœ RSS â†’ ìš”ì•½ â†’ ì¹´í…Œê³ ë¦¬/ê°ì„±/ì ìˆ˜ â†’ íšŒì‚¬ë§¤ì¹­ â†’ ì¹´ë“œ JSON
import os
import re
import math
import json
import logging
import asyncio
import hashlib
import time
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Optional, Set, Tuple, Any
from urllib.parse import urlparse, quote_plus

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
import requests
import feedparser

from modules.translate.libre import translate
from api.storage.pg import connect_db, disconnect_db, init_db, fetch, fetchrow, execute # <--- ADD THIS LINE

# ê°„ë‹¨í•œ TTL ìºì‹œ êµ¬í˜„ (cachetools ëŒ€ì²´)
class SimpleCache:
    def __init__(self, max_size: int = 1000, ttl: int = 300):
        self.cache = {}
        self.timestamps = {}
        self.max_size = max_size
        self.ttl = ttl
    
    def get(self, key: str, default=None):
        if key in self.cache:
            if time.time() - self.timestamps[key] < self.ttl:
                return self.cache[key]
            else:
                # ë§Œë£Œëœ í•­ëª© ì œê±°
                del self.cache[key]
                del self.timestamps[key]
        return default
    
    def set(self, key: str, value: Any):
        # ìºì‹œ í¬ê¸° ì œí•œ
        if len(self.cache) >= self.max_size:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
            oldest_key = min(self.timestamps.keys(), key=lambda k: self.timestamps[k])
            del self.cache[oldest_key]
            del self.timestamps[oldest_key]
        
        self.cache[key] = value
        self.timestamps[key] = time.time()
    
    def __contains__(self, key: str) -> bool:
        return self.get(key) is not None
    
    def __len__(self) -> int:
        return len(self.cache)
    
    def clear(self):
        self.cache.clear()
        self.timestamps.clear()

# ---------------------- ì„¤ì • ë° ìƒìˆ˜ ----------------------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
WEB_DIR = os.path.join(BASE_DIR, "web")
DATA_DIR = os.path.join(BASE_DIR, "data")

# í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì • ê°€ëŠ¥í•˜ë„ë¡ ê°œì„ 
USE_SAMPLE = os.getenv("USE_SAMPLE", "false").lower() == "true"
REQUEST_TIMEOUT = int(os.getenv("REQUEST_TIMEOUT", "8"))
MAX_CONCURRENT_REQUESTS = int(os.getenv("MAX_CONCURRENT_REQUESTS", "5"))
CACHE_TTL = int(os.getenv("CACHE_TTL", "300"))  # 5ë¶„

# RSS í”¼ë“œ ì„¤ì •
RSS_LIST = [
    "https://techcrunch.com/feed/",
    "https://feeds.arstechnica.com/arstechnica/technology-lab",
    "https://www.theverge.com/rss/index.xml",
    "https://www.engadget.com/rss.xml",
]

# í•„í„°ë§ ì„¤ì •
MIN_CONFIDENCE_DISPLAY = float(os.getenv("MIN_CONFIDENCE_DISPLAY", "0.6"))
REQUIRE_COMPANY_MATCH = os.getenv("REQUIRE_COMPANY_MATCH", "true").lower() == "true"
MIN_SCORE = int(os.getenv("MIN_SCORE", "55"))
ALLOWED_CATEGORIES = {
    "financials", "corporate_action", "regulation",
    "product", "supply_chain", "competition", "general"
}
MIN_ITEMS_TARGET = int(os.getenv("MIN_ITEMS_TARGET", "8"))
MAX_ITEMS_PER_FEED = int(os.getenv("MAX_ITEMS_PER_FEED", "20"))

# ëª…ì„¸ì„œì˜ ë£° ì‚¬ì „ (H-1)
SOURCE_TRUST = {
    "sec.gov": 0.98, "ec.europa.eu": 0.96, "justice.gov": 0.95, "reuters.com": 0.94,
    "bloomberg.com": 0.94, "ft.com": 0.90, "wsj.com": 0.90, "cnbc.com": 0.88,
    "investor.nvidia.com": 0.90, "nvidia.com": 0.88, "techcrunch.com": 0.78,
    "theverge.com": 0.75, "engadget.com": 0.70, "unknown": 0.60
}

IMPACT_HINT = {
    0.30: ["ban","recall","data breach","antitrust probe","guidance cut","downgrade","plant halt","export ban"],
    0.20: ["merger","acquisition","IPO","major partnership","export control","sanction"],
    0.15: ["lawsuit","investigation","shipment delay","supply shortage","production issue"]
}

CATEGORY_WEIGHT = {
    "financials": 1.0, "corporate_action": 0.95, "regulation": 0.90, "supply_chain": 0.80,
    "product": 0.65, "competition": 0.55, "general": 0.40
}

# ì „ì—­ ìºì‹œ
feed_cache = SimpleCache(max_size=1000, ttl=CACHE_TTL)
company_db_cache = SimpleCache(max_size=1, ttl=1800)  # 30ë¶„
watchlist_cache = SimpleCache(max_size=1, ttl=1800)   # 30ë¶„

# ---------------------- ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ----------------------
async def load_watchlist(path: Optional[str] = None) -> Set[str]:
    """ì›Œì¹˜ë¦¬ìŠ¤íŠ¸ë¥¼ ìºì‹œì™€ í•¨ê»˜ ë¡œë“œ"""
    if path is None:
        path = os.path.join(DATA_DIR, "datawatchlist.txt")
    
    # ìºì‹œ í™•ì¸
    cache_key = f"watchlist_{path}"
    cached = watchlist_cache.get(cache_key)
    if cached is not None:
        return cached
    
    watchlist = set()
    try:
        # DBì—ì„œ ì›Œì¹˜ë¦¬ìŠ¤íŠ¸ ë¡œë“œ
        # í˜„ì¬ëŠ” user_idê°€ ì—†ìœ¼ë¯€ë¡œ ì„ì‹œë¡œ 'test@example.com' ì‚¬ìš©
        user = await fetchrow("SELECT id FROM users WHERE email = $1", 'test@example.com')
        if user:
            rows = await fetch("SELECT c.name FROM user_watchlist uw JOIN companies c ON uw.company_id = c.id WHERE uw.user_id = $1", user['id'])
            for row in rows:
                watchlist.add(row['name'].upper())
            logger.info(f"DBì—ì„œ ì›Œì¹˜ë¦¬ìŠ¤íŠ¸ {len(watchlist)}ê°œ ê¸°ì—… ë¡œë“œ.")
        else:
            logger.warning("í…ŒìŠ¤íŠ¸ ì‚¬ìš©ì(test@example.com)ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì›Œì¹˜ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ì‹¤íŒ¨.")
    except Exception as e:
        logger.error(f"ì›Œì¹˜ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    watchlist_cache.set(cache_key, watchlist)
    return watchlist

async def load_company_db(path: Optional[str] = None) -> List[Dict]:
    """íšŒì‚¬ DBë¥¼ ìºì‹œì™€ í•¨ê»˜ ë¡œë“œ"""
    if path is None:
        path = os.path.join(DATA_DIR, "companies_context.json")
    
    # ìºì‹œ í™•ì¸
    cache_key = f"company_db_{path}"
    cached = company_db_cache.get(cache_key)
    if cached is not None:
        return cached
    
    company_db = []
    try:
        # DBì—ì„œ íšŒì‚¬ DB ë¡œë“œ
        rows = await fetch("SELECT id, name, tickers, aliases, context, negative, country FROM companies")
        for row in rows:
            company_db.append({
                "id": row['id'],
                "name": row['name'],
                "tickers": row['tickers'],
                "aliases": row['aliases'],
                "context": row['context'],
                "negative": row['negative'],
                "country": row['country']
            })
        logger.info(f"DBì—ì„œ íšŒì‚¬ DB {len(company_db)}ê°œ í•­ëª© ë¡œë“œ.")
    except Exception as e:
        logger.error(f"íšŒì‚¬ DB ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    company_db_cache.set(cache_key, company_db)
    return company_db

def build_dynamic_feeds_from_watchlist(watchlist: Set[str], max_companies: int = 8) -> List[str]:
    """ì›Œì¹˜ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜ ë™ì  í”¼ë“œ ìƒì„±"""
    feeds = []
    if watchlist:
        for name in list(watchlist)[:max_companies]:
            feeds.append(gnews_rss_for(name))
    return feeds

def gnews_rss_for(query: str, lang: str = "en", region: str = "US") -> str:
    """Google News RSS URL ìƒì„±"""
    q = quote_plus(f'"{query}"')
    return f"https://news.google.com/rss/search?q={q}&hl={lang}&gl={region}&ceid={region}:{lang}"

def pick_source(url: str) -> str:
    """URLì—ì„œ ì†ŒìŠ¤ ë„ë©”ì¸ ì¶”ì¶œ"""
    try:
        parsed = urlparse(url)
        return parsed.hostname or "unknown"
    except Exception:
        return "unknown"

def create_article_id(url: str) -> int:
    """URL ê¸°ë°˜ìœ¼ë¡œ ì¼ê´€ëœ article_id ìƒì„±"""
    return abs(hash(hashlib.md5(url.encode()).hexdigest())) % (10**9)

# ---------------------- í…ìŠ¤íŠ¸ ì²˜ë¦¬ í•¨ìˆ˜ ----------------------
TAG_RE = re.compile(r"<[^>]+>")
WHITESPACE_RE = re.compile(r"\s+")

def strip_html(text: str) -> str:
    """HTML íƒœê·¸ ì œê±° ë° í…ìŠ¤íŠ¸ ì •ê·œí™”"""
    if not text:
        return ""
    clean_text = TAG_RE.sub("", text)
    return WHITESPACE_RE.sub(" ", clean_text).strip()

def summarize_en(text: str, max_sentences: int = 2) -> str:
    """í–¥ìƒëœ ì¶”ì¶œ ìš”ì•½ ì•Œê³ ë¦¬ì¦˜"""
    if not text:
        return ""
    
    text = WHITESPACE_RE.sub(" ", text.strip())
    if len(text) < 100:
        return text[:280]
    
    # ë¬¸ì¥ ë¶„ë¦¬ (ê°œì„ ëœ ì •ê·œí‘œí˜„ì‹)
    sentences = re.split(r'(?<=[.!?])\s+(?=[A-Z])', text)
    sentences = [s.strip() for s in sentences if 20 <= len(s) <= 300]
    
    if not sentences:
        return text[:280]
    
    if len(sentences) <= max_sentences:
        return " ".join(sentences)
    
    # ì ìˆ˜ ê¸°ë°˜ ë¬¸ì¥ ì„ íƒ
    scored_sentences = []
    for sentence in sentences:
        words = re.findall(r'\b[a-zA-Z]{3,}\b', sentence.lower())
        if not words:
            continue
        
        # ì ìˆ˜ ê³„ì‚°: ê³ ìœ  ë‹¨ì–´ ìˆ˜, ê¸¸ì´ ìµœì í™”, í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜
        unique_words = len(set(words))
        length_score = 1 / (1 + abs(len(sentence) - 120) / 50)
        keyword_score = sum(1 for word in words if word in {
            'revenue', 'profit', 'earnings', 'growth', 'launch', 'acquisition',
            'partnership', 'investment', 'market', 'technology', 'development'
        })
        
        total_score = unique_words * length_score + keyword_score * 0.5
        scored_sentences.append((total_score, sentence))
    
    # ìƒìœ„ ë¬¸ì¥ ì„ íƒ
    top_sentences = sorted(scored_sentences, key=lambda x: x[0], reverse=True)
    selected = [s for _, s in top_sentences[:max_sentences]]
    
    return " ".join(selected)

def detect_category(text: str) -> str:
    """í–¥ìƒëœ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
    if not text:
        return "general"
    
    text_lower = text.lower()
    
    # ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ íŒ¨í„´ (ê°€ì¤‘ì¹˜ í¬í•¨)
    category_patterns = {
        "financials": [
            (r'\b(earnings|revenue|profit|loss|guidance|quarter|q[1-4]|ebitda|margin)\b', 3),
            (r'\b(beat|miss|forecast|outlook|financial|results)\b', 2),
            (r'\b(billion|million|percent|\$[\d,]+)\b', 1)
        ],
        "corporate_action": [
            (r'\b(merger|acquisition|acquires?|buyout|m&a|ipo|spin-?off)\b', 3),
            (r'\b(deal|transaction|purchase|stake|investment)\b', 2),
            (r'\b(shareholder|stockholder|board|executive)\b', 1)
        ],
        "regulation": [
            (r'\b(regulation|regulatory|ban|banned|approval|approved|license)\b', 3),
            (r'\b(antitrust|probe|investigation|lawsuit|court|legal)\b', 2),
            (r'\b(compliance|policy|government|authority)\b', 1)
        ],
        "supply_chain": [
            (r'\b(supply|shortage|recall|factory|manufacturing|production)\b', 3),
            (r'\b(shipment|delivery|logistics|inventory|raw materials)\b', 2),
            (r'\b(delay|disruption|capacity|facility)\b', 1)
        ],
        "product": [
            (r'\b(launch|launches|unveil|product|feature|update|release)\b', 3),
            (r'\b(innovation|technology|patent|development|version)\b', 2),
            (r'\b(market|consumer|customer|user)\b', 1)
        ],
        "competition": [
            (r'\b(competitor|competition|rival|market share|competitive)\b', 3),
            (r'\b(challenge|threat|opportunity|advantage)\b', 2),
            (r'\b(industry|sector|segment)\b', 1)
        ]
    }
    
    # ê° ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚°
    category_scores = {}
    for category, patterns in category_patterns.items():
        score = 0
        for pattern, weight in patterns:
            matches = len(re.findall(pattern, text_lower))
            score += matches * weight
        category_scores[category] = score
    
    # ìµœê³  ì ìˆ˜ ì¹´í…Œê³ ë¦¬ ë°˜í™˜
    if category_scores:
        best_category = max(category_scores, key=category_scores.get)
        if category_scores[best_category] > 0:
            return best_category
    
    return "general"

def sentiment_score(text: str) -> float:
    """í–¥ìƒëœ ê°ì„± ë¶„ì„"""
    if not text:
        return 0.0
    
    text_lower = text.lower()
    
    # ê°•í•œ ê¸ì •/ë¶€ì • í‚¤ì›Œë“œ (ê°€ì¤‘ì¹˜ 2)
    strong_positive = ["surge", "soar", "breakthrough", "record-breaking", "outstanding"]
    strong_negative = ["plunge", "crash", "disaster", "bankruptcy", "scandal"]
    
    # ì¼ë°˜ ê¸ì •/ë¶€ì • í‚¤ì›Œë“œ (ê°€ì¤‘ì¹˜ 1)
    positive_words = [
        "growth", "profit", "beat", "strong", "success", "partnership",
        "expansion", "innovation", "approval", "upgrade", "optimistic"
    ]
    negative_words = [
        "loss", "decline", "miss", "weak", "failure", "lawsuit",
        "recall", "downgrade", "concern", "risk", "problem"
    ]
    
    # ì ìˆ˜ ê³„ì‚°
    pos_score = sum(2 for word in strong_positive if word in text_lower)
    pos_score += sum(1 for word in positive_words if word in text_lower)
    
    neg_score = sum(2 for word in strong_negative if word in text_lower)
    neg_score += sum(1 for word in negative_words if word in text_lower)
    
    if pos_score == 0 and neg_score == 0:
        return 0.0
    
    total_score = pos_score - neg_score
    max_possible = max(pos_score + neg_score, 1)
    normalized_score = total_score / max_possible
    
    return max(-1.0, min(1.0, round(normalized_score * 0.8, 2)))

def insight_ko(category: str, sentiment: float) -> str:
    """í•œêµ­ì–´ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
    sentiment_text = "ê¸ì •ì " if sentiment > 0.1 else "ë¶€ì •ì " if sentiment < -0.1 else "ì¤‘ë¦½ì "
    
    insights = {
        "financials": f"ğŸ’¡ ì‹¤ì /ì¬ë¬´ ê´€ë ¨ - {sentiment_text} ì‹œê·¸ë„",
        "corporate_action": f"ğŸ’¡ ê¸°ì—… í™œë™ - êµ¬ì¡°ì  ë³€í™” ì˜ˆìƒ",
        "regulation": f"ğŸ’¡ ê·œì œ/ìŠ¹ì¸ - {sentiment_text} ì˜í–¥ ì „ë§",
        "supply_chain": f"ğŸ’¡ ê³µê¸‰ë§ - ìš´ì˜ íš¨ìœ¨ì„± ê´€ë ¨",
        "product": f"ğŸ’¡ ì œí’ˆ/ê¸°ìˆ  - ì‹œì¥ ë°˜ì‘ ëª¨ë‹ˆí„°ë§",
        "competition": f"ğŸ’¡ ê²½ìŸ í™˜ê²½ - ì‹œì¥ ì ìœ ìœ¨ ë³€ë™ ê°€ëŠ¥",
        "general": f"ğŸ’¡ ì¼ë°˜ - {sentiment_text} ëª¨ë©˜í…€"
    }
    
    return insights.get(category, f"ğŸ’¡ ì¼ë°˜ - {sentiment_text} ëª¨ë©˜í…€")

def invest_score(category: str, sentiment: float, published_at: str) -> int:
    """íˆ¬ì ì ìˆ˜ ê³„ì‚°"""
    # ì¹´í…Œê³ ë¦¬ë³„ ê°€ì¤‘ì¹˜
    category_weights = {
        "financials": 1.0,
        "corporate_action": 0.95,
        "regulation": 0.85,
        "supply_chain": 0.75,
        "product": 0.65,
        "competition": 0.55,
        "general": 0.4
    }
    
    try:
        pub_time = datetime.fromisoformat(published_at.replace("Z", ""))
        if pub_time.tzinfo is None:
            pub_time = pub_time.replace(tzinfo=timezone.utc)
    except Exception:
        pub_time = datetime.now(timezone.utc)
    
    # ì‹œê°„ ê°€ì¤‘ì¹˜ (ìµœê·¼ì¼ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
    hours_ago = (datetime.now(timezone.utc) - pub_time).total_seconds() / 3600
    recency_weight = math.exp(-hours_ago / 48.0)  # 48ì‹œê°„ ë°˜ê°ê¸°
    
    # ê¸°ë³¸ ì ìˆ˜ ê³„ì‚°
    base_score = 50
    category_bonus = category_weights.get(category, 0.4) * 25
    sentiment_bonus = abs(sentiment) * 20
    recency_bonus = recency_weight * 15
    
    total_score = base_score + category_bonus + sentiment_bonus + recency_bonus
    
    return max(0, min(100, int(round(total_score))))

# ---------------------- íšŒì‚¬ ë§¤ì¹­ í•¨ìˆ˜ ----------------------
def count_occurrences(text: str, phrase: str) -> int:
    """ë‹¨ì–´ ê²½ê³„ë¥¼ ê³ ë ¤í•œ êµ¬ë¬¸ ì¶œí˜„ íšŸìˆ˜ ê³„ì‚°"""
    if not text or not phrase:
        return 0
    
    pattern = re.escape(phrase.lower())
    boundary_pattern = rf'\b{pattern}\b'
    
    try:
        return len(re.findall(boundary_pattern, text.lower()))
    except re.error:
        # ì •ê·œí‘œí˜„ì‹ ì˜¤ë¥˜ ì‹œ ë‹¨ìˆœ ê²€ìƒ‰
        return text.lower().count(phrase.lower())

def detect_companies_from_db(title: str, body: str, company_db: List[Dict]) -> List[Dict]:
    """íšŒì‚¬ DB ê¸°ë°˜ ì—”í‹°í‹° ë§¤ì¹­"""
    if not company_db:
        return []
    
    results = []
    full_text = f"{title} {body}".lower()
    
    for company in company_db:
        name = company.get("name", "")
        tickers = company.get("tickers", [])
        aliases = company.get("aliases", [])
        context = company.get("context", [])
        negative = company.get("negative", [])
        
        score = 0.0
        matched_term = ""
        
        # í‹°ì»¤ ë§¤ì¹­ (ìµœê³  ìš°ì„ ìˆœìœ„)
        ticker_hits = 0
        for ticker in tickers:
            hits = count_occurrences(full_text, ticker)
            if hits > 0:
                ticker_hits += hits
                if not matched_term:
                    matched_term = ticker
        
        if ticker_hits > 0:
            score += 6.0  # í‹°ì»¤ëŠ” ë†’ì€ ì ìˆ˜
        
        # ë³„ì¹­ ë§¤ì¹­
        alias_total = 0
        alias_in_title = 0
        for alias in aliases:
            total_hits = count_occurrences(full_text, alias)
            title_hits = count_occurrences(title.lower(), alias)
            
            alias_total += total_hits
            alias_in_title += title_hits
            
            if total_hits > 0 and not matched_term:
                matched_term = alias
        
        if alias_total > 0:
            score += 3.0
        if alias_in_title > 0:
            score += 2.0  # ì œëª©ì— ë‚˜ì˜¨ ê²½ìš° ì¶”ê°€ ì ìˆ˜
        
        # ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ
        context_hits = sum(count_occurrences(full_text, word) for word in context)
        score += min(2.0, context_hits * 0.3)
        
        # ë¶€ì • í‚¤ì›Œë“œ ê°ì 
        negative_hits = sum(count_occurrences(full_text, word) for word in negative)
        score -= negative_hits * 1.5
        
        # ìœ íš¨ì„± ê²€ì‚¬
        is_valid = (
            ticker_hits > 0 or
            alias_in_title > 0 or
            alias_total >= 2 or
            (alias_total >= 1 and context_hits >= 2)
        )
        
        if not is_valid:
            continue
        
        # ì‹ ë¢°ë„ ê³„ì‚°
        confidence = max(0.05, min(1.0, score / 10.0))
        
        if confidence >= MIN_CONFIDENCE_DISPLAY:
            results.append({
                "name": name,
                "confidence": round(confidence, 2),
                "matched": matched_term,
                "country": company.get("country", "")
            })
    
    return sorted(results, key=lambda x: x["confidence"], reverse=True)

# ---------------------- RSS ì²˜ë¦¬ í•¨ìˆ˜ ----------------------
def fetch_rss(url: str):
    """ë™ê¸° RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° (ê¸°ì¡´ ì½”ë“œì™€ í˜¸í™˜)"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (NewsMonitor/2.0) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)
        response.raise_for_status()
        return feedparser.parse(response.content)
    except Exception as e:
        logger.warning(f"RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ {url}: {e}")
        return None

def company_allowed(matches: List[Dict], watchlist: Set[str]) -> bool:
    """íšŒì‚¬ ë§¤ì¹­ì´ ì›Œì¹˜ë¦¬ìŠ¤íŠ¸ì— í—ˆìš©ë˜ëŠ”ì§€ í™•ì¸"""
    if not REQUIRE_COMPANY_MATCH:
        return True
    
    if not matches:
        return False
    
    if not watchlist:  # ì›Œì¹˜ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìœ¼ë©´ ë§¤ì¹­ë§Œ ìˆìœ¼ë©´ í—ˆìš©
        return True
    
    return any(
        match["name"].upper() in watchlist and match["confidence"] >= MIN_CONFIDENCE_DISPLAY
        for match in matches
    )

def passes_filters(item: Dict, require_match: bool = True, min_score: int = MIN_SCORE, 
                  allowed_categories: Set[str] = ALLOWED_CATEGORIES) -> bool:
    """ë‰´ìŠ¤ ì•„ì´í…œì´ í•„í„°ë¥¼ í†µê³¼í•˜ëŠ”ì§€ í™•ì¸"""
    if require_match and not item.get("companies"):
        return False
    
    if min_score and item.get("score", 0) < min_score:
        return False
    
    if allowed_categories and item.get("category") not in allowed_categories:
        return False
    
    return True

def sample_items() -> List[Dict]:
    """ìƒ˜í”Œ ë‰´ìŠ¤ ì•„ì´í…œ ìƒì„±"""
    now = datetime.now(timezone.utc).isoformat()
    return [
        {
            "article_id": 1,
            "title": "ğŸ“° ìƒ˜í”Œ: ì™¸ë¶€ RSS ì‘ë‹µ ì§€ì—° ì‹œ í‘œì‹œë©ë‹ˆë‹¤",
            "source": "local",
            "published_at": now,
            "ko_short": "ë„¤íŠ¸ì›Œí¬/ë°©í™”ë²½/í”¼ë“œ ì§€ì—°ìœ¼ë¡œ ì„ì‹œ ìƒ˜í”Œì„ ë…¸ì¶œí•©ë‹ˆë‹¤.",
            "ko_insight": "ğŸ’¡ í™˜ê²½ í™•ì¸ í›„ ì •ìƒí™”ë˜ë©´ ìë™ìœ¼ë¡œ ì‹¤ê¸°ì‚¬ë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤.",
            "score": 40,
            "sentiment": 0.0,
            "category": "general",
            "company_name": "",
            "companies": [],
            "url": "#",
        }
    ]

# ---------------------- FastAPI ì„¤ì • ----------------------
app = FastAPI(
    title="NewsMonitor API",
    description="AI ê¸°ë°˜ ê¸°ì—… ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ",
    version="2.0.0"
)

# ë¯¸ë“¤ì›¨ì–´ ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
app.add_middleware(GZipMiddleware, minimum_size=1000)

@app.on_event("startup")
async def startup_event():
    print("Application startup...")
    await connect_db()
    # ê°œë°œ í™˜ê²½ì—ì„œë§Œ ìŠ¤í‚¤ë§ˆ ì ìš© ë° ì´ˆê¸° ë°ì´í„° ì‚½ì…
    if os.getenv("APP_ENV", "development") == "development":
        await init_db()
    print("Database connected and initialized.")

@app.on_event("shutdown")
async def shutdown_event():
    print("Application shutdown...")
    await disconnect_db()
    print("Database disconnected.")

# ì •ì  íŒŒì¼ ì„œë¹™
app.mount("/static", StaticFiles(directory=WEB_DIR), name="static")

# ---------------------- ë¼ìš°íŠ¸ ----------------------
@app.get("/")
def home():
    """í™ˆí˜ì´ì§€ ë°˜í™˜"""
    return FileResponse(os.path.join(WEB_DIR, "index.html"))

@app.get("/health")
def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "status": "healthy",
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "version": "2.0.0"
    }

@app.post("/admin/reload_companies")
def reload_companies():
    """íšŒì‚¬ DB ì¬ë¡œë”©"""
    try:
        company_db_cache.clear()
        company_db = load_company_db()
        return {
            "success": True,
            "count": len(company_db),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
    except Exception as e:
        logger.error(f"íšŒì‚¬ DB ì¬ë¡œë”© ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/admin/reload_watchlist")
def reload_watchlist():
    """ì›Œì¹˜ë¦¬ìŠ¤íŠ¸ ì¬ë¡œë”©"""
    try:
        watchlist_cache.clear()
        watchlist = load_watchlist()
        return {
            "success": True,
            "count": len(watchlist),
            "companies": list(watchlist)[:10],  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
    except Exception as e:
        logger.error(f"ì›Œì¹˜ë¦¬ìŠ¤íŠ¸ ì¬ë¡œë”© ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/feed")
async def get_feed(limit: int = 20):
    """ë©”ì¸ ë‰´ìŠ¤ í”¼ë“œ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        # ìºì‹œ í™•ì¸
        cache_key = f"feed_{limit}"
        cached_result = feed_cache.get(cache_key)
        if cached_result is not None:
            logger.info(f"ìºì‹œì—ì„œ í”¼ë“œ ë°˜í™˜: {len(cached_result)} ì•„ì´í…œ")
            return cached_result[:limit]
        
        if USE_SAMPLE:
            logger.info("ìƒ˜í”Œ ëª¨ë“œ í™œì„±í™”")
            return sample_items()[:limit]
        
        # ë°ì´í„° ë¡œë“œ
        watchlist = await load_watchlist()
        company_db = await load_company_db()
        
        # RSS í”¼ë“œ ìˆ˜ì§‘
        all_feeds = list(RSS_LIST) + build_dynamic_feeds_from_watchlist(watchlist)
        logger.info(f"{len(all_feeds)}ê°œ RSS í”¼ë“œ ì²˜ë¦¬ ì‹œì‘")
        
        candidates = []
        seen_urls = set()
        
        # RSS í”¼ë“œ ì²˜ë¦¬ (ë™ê¸°ì‹, ê¸°ì¡´ ì½”ë“œì™€ í˜¸í™˜)
        for feed_url in all_feeds:
            try:
                feed_data = fetch_rss(feed_url)
                if not feed_data:
                    continue
                
                # í”¼ë“œ ì—”íŠ¸ë¦¬ ì²˜ë¦¬
                for entry in feed_data.entries[:MAX_ITEMS_PER_FEED]:
                    try:
                        title = entry.get("title", "")
                        link = entry.get("link", "#")
                        
                        if not link or link in seen_urls:
                            continue
                        seen_urls.add(link)
                        
                        # ë°œí–‰ ì‹œê°„ ì²˜ë¦¬
                        pub_time = getattr(entry, "published_parsed", None) or getattr(entry, "updated_parsed", None)
                        if pub_time:
                            published_at = datetime(*pub_time[:6], tzinfo=timezone.utc).isoformat()
                        else:
                            published_at = datetime.now(timezone.utc).isoformat()
                        
                        # ë³¸ë¬¸ ì¶”ì¶œ
                        desc_raw = entry.get("summary", "") or entry.get("description", "")
                        desc = strip_html(desc_raw)
                        full_text = f"{title}. {desc}".strip()
                        
                        # AI ë¶„ì„
                        matches = detect_companies_from_db(title, desc, company_db)
                        top_company = matches[0]["name"] if matches else ""
                        
                        summary = summarize_en(desc or title, max_sentences=2)
                        
                        # Translate the English summary to Korean
                        korean_summary = summary
                        if summary:
                            korean_summary = translate(summary, source='en', target='ko')

                        category = detect_category(full_text)
                        sentiment = sentiment_score(full_text)
                        insight = insight_ko(category, sentiment)
                        score = invest_score(category, sentiment, published_at)
                        
                        # ë‰´ìŠ¤ ì•„ì´í…œ ìƒì„±
                        news_item = {
                            "article_id": create_article_id(link),
                            "title": title,
                            "source": pick_source(link),
                            "published_at": published_at,
                            "ko_short": korean_summary, # USE korean_summary HERE
                            "ko_insight": insight,
                            "score": score,
                            "sentiment": sentiment,
                            "category": category,
                            "company_name": top_company,
                            "companies": matches,
                            "url": link,
                        }
                        
                        candidates.append(news_item)
                        
                    except Exception as e:
                        logger.warning(f"ë‰´ìŠ¤ ì•„ì´í…œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        continue
                        
            except Exception as e:
                logger.warning(f"RSS í”¼ë“œ ì²˜ë¦¬ ì‹¤íŒ¨ {feed_url}: {e}")
                continue
        
        logger.info(f"ì´ {len(candidates)}ê°œ í›„ë³´ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
        
        # 1ì°¨ í•„í„°ë§: ì—„ê²©í•œ ê¸°ì¤€
        strict_filtered = [
            item for item in candidates 
            if passes_filters(item, require_match=True, min_score=MIN_SCORE, 
                            allowed_categories=ALLOWED_CATEGORIES) 
            and company_allowed(item.get("companies", []), watchlist)
        ]
        
        # 2ì°¨ í•„í„°ë§: ê²°ê³¼ê°€ ë¶€ì¡±í•  ê²½ìš° ì™„í™”ëœ ê¸°ì¤€ ì ìš©
        if len(strict_filtered) < MIN_ITEMS_TARGET:
            logger.info(f"1ì°¨ í•„í„°ë§ ê²°ê³¼ ë¶€ì¡± ({len(strict_filtered)}ê°œ), 2ì°¨ í•„í„°ë§ ì ìš©")
            relaxed_filtered = [
                item for item in candidates 
                if passes_filters(item, require_match=False, min_score=min(45, MIN_SCORE), 
                                allowed_categories=None)
            ]
            
            # ì¤‘ë³µ ì œê±° í›„ ê²°í•©
            seen_ids = {item["article_id"] for item in strict_filtered}
            additional_items = [
                item for item in relaxed_filtered 
                if item["article_id"] not in seen_ids
            ]
            
            final_results = strict_filtered + additional_items
        else:
            final_results = strict_filtered
        
        # ì •ë ¬: ìµœì‹ ìˆœ, ë™ì¼ ì‹œê°„ì¼ ê²½ìš° ì ìˆ˜ìˆœ
        final_results.sort(key=lambda x: (x["published_at"], x["score"]), reverse=True)
        
        # ê²°ê³¼ ì œí•œ
        limited_results = final_results[:limit]
        
        # ìºì‹œì— ì €ì¥
        feed_cache.set(cache_key, limited_results)
        
        # ë¹ˆ ê²°ê³¼ì¼ ê²½ìš° ìƒ˜í”Œ ë°˜í™˜
        if not limited_results:
            logger.warning("í•„í„°ë§ í›„ ê²°ê³¼ê°€ ì—†ìŒ, ìƒ˜í”Œ ë°˜í™˜")
            return sample_items()[:limit]
        
        logger.info(f"ìµœì¢… {len(limited_results)}ê°œ ë‰´ìŠ¤ ë°˜í™˜")
        return limited_results
        
    except Exception as e:
        logger.exception("ë‰´ìŠ¤ í”¼ë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
        
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ìºì‹œëœ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ë°˜í™˜
        cache_key = f"feed_{limit}"
        cached_result = feed_cache.get(cache_key)
        if cached_result is not None:
            logger.info("ì˜¤ë¥˜ ë°œìƒ, ìºì‹œëœ ê²°ê³¼ ë°˜í™˜")
            return cached_result
        
        # ëª¨ë“  ê²ƒì´ ì‹¤íŒ¨í•˜ë©´ ìƒ˜í”Œ ë°˜í™˜
        logger.info("ëª¨ë“  ì²˜ë¦¬ ì‹¤íŒ¨, ìƒ˜í”Œ ë°˜í™˜")
        return sample_items()[:limit]

@app.get("/feed/categories")
def get_categories():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬ ëª©ë¡ ë°˜í™˜"""
    return {
        "categories": list(ALLOWED_CATEGORIES),
        "descriptions": {
            "financials": "ì‹¤ì , ë§¤ì¶œ, ìˆ˜ìµ ê´€ë ¨",
            "corporate_action": "ì¸ìˆ˜í•©ë³‘, IPO, êµ¬ì¡°ì¡°ì •",
            "regulation": "ê·œì œ, ìŠ¹ì¸, ë²•ì  ì´ìŠˆ",
            "product": "ì œí’ˆ ì¶œì‹œ, ê¸°ìˆ  ê°œë°œ",
            "supply_chain": "ê³µê¸‰ë§, ìƒì‚°, ë¦¬ì½œ",
            "competition": "ê²½ìŸ, ì‹œì¥ ì ìœ ìœ¨",
            "general": "ì¼ë°˜ ë‰´ìŠ¤"
        }
    }

@app.get("/feed/sources")
def get_sources():
    """ë‰´ìŠ¤ ì†ŒìŠ¤ ëª©ë¡ ë°˜í™˜"""
    watchlist = load_watchlist()
    
    return {
        "rss_feeds": RSS_LIST,
        "dynamic_feeds_count": len(build_dynamic_feeds_from_watchlist(watchlist)),
        "watchlist_companies": len(watchlist)
    }

@app.get("/admin/stats")
def get_stats():
    """ì‹œìŠ¤í…œ í†µê³„ ë°˜í™˜"""
    watchlist = load_watchlist()
    company_db = load_company_db()
    
    return {
        "cache_stats": {
            "feed_cache_size": len(feed_cache),
            "company_db_cached": len(company_db_cache),
            "watchlist_cached": len(watchlist_cache)
        },
        "data_stats": {
            "companies_in_db": len(company_db),
            "watchlist_size": len(watchlist),
            "rss_feeds": len(RSS_LIST)
        },
        "config": {
            "min_confidence": MIN_CONFIDENCE_DISPLAY,
            "min_score": MIN_SCORE,
            "max_items_per_feed": MAX_ITEMS_PER_FEED,
            "cache_ttl_seconds": CACHE_TTL,
            "request_timeout": REQUEST_TIMEOUT
        },
        "timestamp": datetime.now(timezone.utc).isoformat()
    }

@app.get("/companies/search")
def search_companies(q: str):
    """íšŒì‚¬ ê²€ìƒ‰"""
    if len(q.strip()) < 2:
        return {"results": []}
    
    company_db = load_company_db()
    query = q.lower().strip()
    
    matches = []
    for company in company_db:
        name = company.get("name", "").lower()
        tickers = [t.lower() for t in company.get("tickers", [])]
        aliases = [a.lower() for a in company.get("aliases", [])]
        
        score = 0
        if query in name:
            score += 10
        if any(query in ticker for ticker in tickers):
            score += 8
        if any(query in alias for alias in aliases):
            score += 6
        
        if score > 0:
            matches.append({
                "name": company.get("name"),
                "tickers": company.get("tickers", []),
                "country": company.get("country", ""),
                "score": score
            })
    
    # ì ìˆ˜ìˆœ ì •ë ¬, ìƒìœ„ 10ê°œë§Œ ë°˜í™˜
    matches.sort(key=lambda x: x["score"], reverse=True)
    return {"results": matches[:10]}

if __name__ == "__main__":
    import uvicorn
    
    # ê°œë°œ í™˜ê²½ì—ì„œë§Œ ì‚¬ìš©
    uvicorn.run(
        "server:app",
        host="0.0.0.0",
        port=8000,
        reload=False,  # ì•ˆì •ì„±ì„ ìœ„í•´ reload=Falseë¡œ ì„¤ì •
        log_level="info"
    )
                